# æœ€ä½³å®è·µæŒ‡å— Best Practices Guide

æœ¬æŒ‡å—æä¾›ä½¿ç”¨Markdown Translatorçš„æœ€ä½³å®è·µï¼Œå¸®åŠ©æ‚¨è·å¾—æœ€ä½³çš„ç¿»è¯‘è´¨é‡å’Œæ€§èƒ½ã€‚

This guide provides best practices for using Markdown Translator to achieve optimal translation quality and performance.

## ğŸ¯ ç¿»è¯‘è´¨é‡ä¼˜åŒ– Translation Quality Optimization

### 1. é€‰æ‹©åˆé€‚çš„æ¨¡å‹ Choose the Right Model

#### é«˜è´¨é‡ç¿»è¯‘ High Quality Translation
```bash
# é€‚ç”¨äºé‡è¦æ–‡æ¡£ã€æŠ€æœ¯æ–‡æ¡£ã€æ­£å¼å‘å¸ƒå†…å®¹
# For important documents, technical docs, official releases
export TRANSLATE_MODEL="claude-3-5-sonnet-20241022"
markdown-translator -i important_doc.md -c 200 -n 2 --verbose
```

#### å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦ Balance Quality and Speed
```bash
# é€‚ç”¨äºæ—¥å¸¸æ–‡æ¡£ã€åšå®¢æ–‡ç« ã€å†…éƒ¨æ–‡æ¡£
# For daily docs, blog posts, internal documentation
export TRANSLATE_MODEL="qwen/qwen-2.5-72b-instruct"
markdown-translator -i regular_doc.md -c 400 -n 5
```

#### å¿«é€Ÿç¿»è¯‘ Fast Translation
```bash
# é€‚ç”¨äºè‰ç¨¿ã€ä¸´æ—¶æ–‡æ¡£ã€å¤§æ‰¹é‡å¤„ç†
# For drafts, temporary docs, bulk processing
export TRANSLATE_MODEL="qwen/qwen-2.5-7b-instruct"
markdown-translator -i draft_doc.md -c 800 -n 8
```

### 2. ä¼˜åŒ–åˆ†å—ç­–ç•¥ Optimize Chunking Strategy

#### æ ¹æ®å†…å®¹ç±»å‹è°ƒæ•´ Adjust Based on Content Type

```bash
# æŠ€æœ¯æ–‡æ¡£ï¼ˆä¿æŒæœ¯è¯­ä¸€è‡´æ€§ï¼‰Technical docs (maintain terminology consistency)
markdown-translator -i tech_doc.md -c 150 -n 2

# APIæ–‡æ¡£ï¼ˆä¿æŒä»£ç å®Œæ•´æ€§ï¼‰API docs (maintain code integrity)
markdown-translator -i api_doc.md -c 100 -n 1

# åšå®¢æ–‡ç« ï¼ˆå¹³è¡¡ä¸Šä¸‹æ–‡å’Œæ•ˆç‡ï¼‰Blog posts (balance context and efficiency)
markdown-translator -i blog_post.md -c 400 -n 5

# READMEæ–‡ä»¶ï¼ˆå¿«é€Ÿå¤„ç†ï¼‰README files (quick processing)
markdown-translator -i README.md -c 600 -n 6
```

#### æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´ Adjust Based on File Size

```bash
# å°æ–‡ä»¶ < 500è¡Œ Small files < 500 lines
markdown-translator -i small.md -c 200 -n 2

# ä¸­ç­‰æ–‡ä»¶ 500-2000è¡Œ Medium files 500-2000 lines
markdown-translator -i medium.md -c 400 -n 4

# å¤§æ–‡ä»¶ > 2000è¡Œ Large files > 2000 lines
markdown-translator -i large.md -c 800 -n 6
```

### 3. é¢„å¤„ç†æ–‡æ¡£ Preprocess Documents

#### æ¸…ç†æ ¼å¼ Clean Formatting
```bash
# ç§»é™¤å¤šä½™ç©ºè¡Œ Remove extra blank lines
sed '/^$/N;/^\n$/d' input.md > cleaned.md

# ç»Ÿä¸€ä»£ç å—æ ‡è®° Standardize code block markers
sed 's/~~~python/```python/g' input.md > standardized.md

# ä¿®å¤è¡¨æ ¼æ ¼å¼ Fix table formatting
# ç¡®ä¿è¡¨æ ¼è¡Œå¯¹é½æ­£ç¡®
```

#### æ£€æŸ¥Markdownè¯­æ³• Check Markdown Syntax
```bash
# ä½¿ç”¨markdownlintæ£€æŸ¥è¯­æ³• Use markdownlint to check syntax
npm install -g markdownlint-cli
markdownlint input.md

# ä¿®å¤å¸¸è§é—®é¢˜ Fix common issues
markdownlint --fix input.md
```

## âš¡ æ€§èƒ½ä¼˜åŒ– Performance Optimization

### 1. å¹¶å‘é…ç½®ä¼˜åŒ– Concurrency Configuration Optimization

#### ç½‘ç»œç¯å¢ƒè¯„ä¼° Network Environment Assessment
```bash
# æµ‹è¯•ç½‘ç»œå»¶è¿Ÿ Test network latency
ping -c 10 openrouter.ai

# æµ‹è¯•å¸¦å®½ Test bandwidth
curl -w "@curl-format.txt" -o /dev/null -s https://openrouter.ai/api/v1/models

# åˆ›å»ºcurlæ ¼å¼æ–‡ä»¶ Create curl format file
cat > curl-format.txt << 'EOF'
     time_namelookup:  %{time_namelookup}\n
        time_connect:  %{time_connect}\n
     time_appconnect:  %{time_appconnect}\n
    time_pretransfer:  %{time_pretransfer}\n
       time_redirect:  %{time_redirect}\n
  time_starttransfer:  %{time_starttransfer}\n
                     ----------\n
          time_total:  %{time_total}\n
EOF
```

#### åŠ¨æ€è°ƒæ•´å¹¶å‘æ•° Dynamically Adjust Concurrency
```bash
# å¿«é€Ÿç½‘ç»œï¼ˆå»¶è¿Ÿ < 100msï¼‰Fast network (latency < 100ms)
markdown-translator -i file.md -n 8

# ä¸­ç­‰ç½‘ç»œï¼ˆå»¶è¿Ÿ 100-300msï¼‰Medium network (latency 100-300ms)
markdown-translator -i file.md -n 4

# æ…¢é€Ÿç½‘ç»œï¼ˆå»¶è¿Ÿ > 300msï¼‰Slow network (latency > 300ms)
markdown-translator -i file.md -n 2
```

### 2. å†…å­˜ç®¡ç† Memory Management

#### ç›‘æ§å†…å­˜ä½¿ç”¨ Monitor Memory Usage
```bash
# å®æ—¶ç›‘æ§å†…å­˜ Real-time memory monitoring
watch -n 1 'ps aux | grep markdown-translator | grep -v grep'

# è®¾ç½®å†…å­˜é™åˆ¶ Set memory limits
ulimit -v 2097152  # é™åˆ¶è™šæ‹Ÿå†…å­˜ä¸º2GB Limit virtual memory to 2GB
markdown-translator -i large_file.md -c 400 -n 3
```

#### å¤§æ–‡ä»¶å¤„ç†ç­–ç•¥ Large File Processing Strategy
```bash
# æ–¹æ³•1ï¼šå‡å°åˆ†å—å¤§å° Method 1: Reduce chunk size
markdown-translator -i huge_file.md -c 200 -n 2

# æ–¹æ³•2ï¼šåˆ†å‰²æ–‡ä»¶å¤„ç† Method 2: Split file processing
split -l 2000 huge_file.md part_
for part in part_*; do
    markdown-translator -i "$part" -o "${part}_zh.md" -c 500 -n 4
done
cat part_*_zh.md > huge_file_zh.md

# æ–¹æ³•3ï¼šæµå¼å¤„ç† Method 3: Streaming processing
# ä½¿ç”¨è‡ªå®šä¹‰è„šæœ¬é€æ®µå¤„ç†
```

### 3. ç¼“å­˜ç­–ç•¥ Caching Strategy

#### é¿å…é‡å¤ç¿»è¯‘ Avoid Duplicate Translation
```bash
# åˆ›å»ºç¿»è¯‘ç¼“å­˜è„šæœ¬ Create translation cache script
cat > cached_translate.sh << 'EOF'
#!/bin/bash

INPUT_FILE="$1"
OUTPUT_FILE="$2"
CACHE_DIR=".translation_cache"

# åˆ›å»ºç¼“å­˜ç›®å½• Create cache directory
mkdir -p "$CACHE_DIR"

# è®¡ç®—æ–‡ä»¶å“ˆå¸Œ Calculate file hash
HASH=$(md5sum "$INPUT_FILE" | cut -d' ' -f1)
CACHE_FILE="$CACHE_DIR/$HASH.md"

if [ -f "$CACHE_FILE" ]; then
    echo "Using cached translation for $INPUT_FILE"
    cp "$CACHE_FILE" "$OUTPUT_FILE"
else
    echo "Translating $INPUT_FILE"
    markdown-translator -i "$INPUT_FILE" -o "$OUTPUT_FILE"
    cp "$OUTPUT_FILE" "$CACHE_FILE"
fi
EOF

chmod +x cached_translate.sh
./cached_translate.sh input.md output_zh.md
```

## ğŸ”’ å®‰å…¨æœ€ä½³å®è·µ Security Best Practices

### 1. APIå¯†é’¥ç®¡ç† API Key Management

#### å®‰å…¨å­˜å‚¨ Secure Storage
```bash
# ä½¿ç”¨ä¸“ç”¨çš„ç¯å¢ƒæ–‡ä»¶ Use dedicated environment file
cat > .env.local << 'EOF'
TRANSLATE_API_TOKEN=sk-or-v1-your-secret-key
TRANSLATE_MODEL=qwen/qwen-2.5-72b-instruct
EOF

# è®¾ç½®ä¸¥æ ¼æƒé™ Set strict permissions
chmod 600 .env.local

# åŠ è½½ç¯å¢ƒå˜é‡ Load environment variables
set -a; source .env.local; set +a
```

#### å¯†é’¥è½®æ¢ Key Rotation
```bash
# å®šæœŸæ›´æ¢APIå¯†é’¥ Regularly rotate API keys
# 1. åœ¨OpenRouterç”Ÿæˆæ–°å¯†é’¥
# 2. æ›´æ–°ç¯å¢ƒå˜é‡
# 3. æµ‹è¯•æ–°å¯†é’¥
# 4. æ’¤é”€æ—§å¯†é’¥

# æµ‹è¯•æ–°å¯†é’¥ Test new key
export TRANSLATE_API_TOKEN="new-key"
markdown-translator -i test.md --dry-run
```

### 2. æ–‡ä»¶å®‰å…¨ File Security

#### è¾“å…¥éªŒè¯ Input Validation
```bash
# éªŒè¯æ–‡ä»¶ç±»å‹ Validate file type
file_type=$(file -b --mime-type "$INPUT_FILE")
if [[ "$file_type" != "text/plain" && "$file_type" != "text/markdown" ]]; then
    echo "Error: Invalid file type: $file_type"
    exit 1
fi

# æ£€æŸ¥æ–‡ä»¶å¤§å° Check file size
max_size=$((100 * 1024 * 1024))  # 100MB
file_size=$(stat -c%s "$INPUT_FILE")
if [ "$file_size" -gt "$max_size" ]; then
    echo "Error: File too large: $file_size bytes"
    exit 1
fi
```

#### è·¯å¾„å®‰å…¨ Path Security
```bash
# ä½¿ç”¨ç»å¯¹è·¯å¾„ Use absolute paths
INPUT_FILE=$(realpath "$1")
OUTPUT_FILE=$(realpath "$2")

# éªŒè¯è·¯å¾„åœ¨å…è®¸çš„ç›®å½•å†… Verify paths are within allowed directories
ALLOWED_DIR=$(realpath ~/documents)
if [[ "$INPUT_FILE" != "$ALLOWED_DIR"* ]]; then
    echo "Error: Input file outside allowed directory"
    exit 1
fi
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿— Monitoring and Logging

### 1. æ€§èƒ½ç›‘æ§ Performance Monitoring

#### åˆ›å»ºç›‘æ§è„šæœ¬ Create Monitoring Script
```bash
cat > monitor_translation.sh << 'EOF'
#!/bin/bash

LOG_FILE="translation_monitor.log"
INPUT_FILE="$1"

echo "=== Translation Monitor Started: $(date) ===" >> "$LOG_FILE"
echo "Input file: $INPUT_FILE" >> "$LOG_FILE"
echo "File size: $(du -h "$INPUT_FILE" | cut -f1)" >> "$LOG_FILE"

# è®°å½•å¼€å§‹æ—¶é—´ Record start time
START_TIME=$(date +%s)

# ç›‘æ§ç³»ç»Ÿèµ„æº Monitor system resources
(
    while true; do
        echo "$(date): CPU: $(top -bn1 | grep "Cpu(s)" | awk '{print $2}'), Memory: $(free | grep Mem | awk '{printf "%.1f%%", $3/$2 * 100.0}')" >> "$LOG_FILE"
        sleep 10
    done
) &
MONITOR_PID=$!

# æ‰§è¡Œç¿»è¯‘ Execute translation
markdown-translator -i "$INPUT_FILE" --verbose 2>&1 | tee -a "$LOG_FILE"
TRANSLATION_EXIT_CODE=$?

# åœæ­¢ç›‘æ§ Stop monitoring
kill $MONITOR_PID 2>/dev/null

# è®°å½•ç»“æŸæ—¶é—´ Record end time
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))

echo "=== Translation Completed: $(date) ===" >> "$LOG_FILE"
echo "Duration: ${DURATION}s" >> "$LOG_FILE"
echo "Exit code: $TRANSLATION_EXIT_CODE" >> "$LOG_FILE"
EOF

chmod +x monitor_translation.sh
./monitor_translation.sh input.md
```

### 2. é”™è¯¯è·Ÿè¸ª Error Tracking

#### ç»“æ„åŒ–æ—¥å¿— Structured Logging
```bash
# åˆ›å»ºç»“æ„åŒ–æ—¥å¿—è„šæœ¬ Create structured logging script
cat > structured_translate.sh << 'EOF'
#!/bin/bash

INPUT_FILE="$1"
OUTPUT_FILE="$2"
LOG_FILE="translation_$(date +%Y%m%d_%H%M%S).json"

# è®°å½•å¼€å§‹äº‹ä»¶ Log start event
cat >> "$LOG_FILE" << EOF
{
  "timestamp": "$(date -Iseconds)",
  "event": "translation_started",
  "input_file": "$INPUT_FILE",
  "output_file": "$OUTPUT_FILE",
  "file_size": $(stat -c%s "$INPUT_FILE"),
  "line_count": $(wc -l < "$INPUT_FILE")
}
EOF

# æ‰§è¡Œç¿»è¯‘å¹¶æ•è·ç»“æœ Execute translation and capture results
if markdown-translator -i "$INPUT_FILE" -o "$OUTPUT_FILE" --verbose 2>&1; then
    STATUS="success"
    EXIT_CODE=0
else
    STATUS="failed"
    EXIT_CODE=$?
fi

# è®°å½•å®Œæˆäº‹ä»¶ Log completion event
cat >> "$LOG_FILE" << EOF
{
  "timestamp": "$(date -Iseconds)",
  "event": "translation_completed",
  "status": "$STATUS",
  "exit_code": $EXIT_CODE,
  "output_size": $(stat -c%s "$OUTPUT_FILE" 2>/dev/null || echo 0)
}
EOF
EOF

chmod +x structured_translate.sh
./structured_translate.sh input.md output_zh.md
```

## ğŸ”„ æ‰¹é‡å¤„ç†æœ€ä½³å®è·µ Batch Processing Best Practices

### 1. å¹¶è¡Œæ‰¹é‡å¤„ç† Parallel Batch Processing

#### GNU Parallel æ–¹æ¡ˆ GNU Parallel Solution
```bash
# å®‰è£…GNU parallel Install GNU parallel
# Ubuntu/Debian: sudo apt-get install parallel
# macOS: brew install parallel

# åˆ›å»ºå¹¶è¡Œå¤„ç†å‡½æ•° Create parallel processing function
translate_single() {
    local input_file="$1"
    local output_file="${input_file%.*}_zh.md"
    
    echo "Processing: $input_file"
    
    # æ ¹æ®æ–‡ä»¶å¤§å°è°ƒæ•´å‚æ•° Adjust parameters based on file size
    local file_size=$(stat -c%s "$input_file")
    local chunk_size=500
    local concurrency=3
    
    if [ "$file_size" -gt 1048576 ]; then  # > 1MB
        chunk_size=800
        concurrency=2
    fi
    
    markdown-translator -i "$input_file" -o "$output_file" \
                       -c "$chunk_size" -n "$concurrency"
}

export -f translate_single
export TRANSLATE_API_TOKEN TRANSLATE_MODEL

# å¹¶è¡Œå¤„ç†æ‰€æœ‰æ–‡ä»¶ Process all files in parallel
find docs -name "*.md" | parallel -j 4 translate_single {}
```

#### é˜Ÿåˆ—ç®¡ç† Queue Management
```bash
# åˆ›å»ºä»»åŠ¡é˜Ÿåˆ—ç®¡ç†å™¨ Create task queue manager
cat > queue_manager.sh << 'EOF'
#!/bin/bash

QUEUE_FILE="translation_queue.txt"
WORKERS=4
WORKER_PIDS=()

# åˆ›å»ºé˜Ÿåˆ—æ–‡ä»¶ Create queue file
find docs -name "*.md" > "$QUEUE_FILE"

# å·¥ä½œè¿›ç¨‹å‡½æ•° Worker process function
worker() {
    local worker_id="$1"
    local log_file="worker_${worker_id}.log"
    
    while true; do
        # ä»é˜Ÿåˆ—è·å–ä»»åŠ¡ Get task from queue
        local task=$(head -n1 "$QUEUE_FILE" 2>/dev/null)
        if [ -z "$task" ]; then
            break
        fi
        
        # ä»é˜Ÿåˆ—ç§»é™¤ä»»åŠ¡ Remove task from queue
        sed -i '1d' "$QUEUE_FILE"
        
        echo "Worker $worker_id processing: $task" | tee -a "$log_file"
        
        # æ‰§è¡Œç¿»è¯‘ Execute translation
        if markdown-translator -i "$task" -c 400 -n 2 2>&1 | tee -a "$log_file"; then
            echo "Worker $worker_id completed: $task" | tee -a "$log_file"
        else
            echo "Worker $worker_id failed: $task" | tee -a "$log_file"
            # é‡æ–°åŠ å…¥é˜Ÿåˆ— Re-add to queue
            echo "$task" >> "$QUEUE_FILE"
        fi
    done
}

# å¯åŠ¨å·¥ä½œè¿›ç¨‹ Start worker processes
for i in $(seq 1 $WORKERS); do
    worker "$i" &
    WORKER_PIDS+=($!)
done

# ç­‰å¾…æ‰€æœ‰å·¥ä½œè¿›ç¨‹å®Œæˆ Wait for all workers to complete
for pid in "${WORKER_PIDS[@]}"; do
    wait "$pid"
done

echo "All translations completed!"
EOF

chmod +x queue_manager.sh
./queue_manager.sh
```

### 2. å¢é‡å¤„ç† Incremental Processing

#### åŸºäºæ—¶é—´æˆ³çš„å¢é‡æ›´æ–° Timestamp-based Incremental Updates
```bash
cat > incremental_translate.sh << 'EOF'
#!/bin/bash

SOURCE_DIR="docs"
TARGET_DIR="docs_zh"
TIMESTAMP_FILE=".last_translation"

# è·å–ä¸Šæ¬¡ç¿»è¯‘æ—¶é—´ Get last translation time
if [ -f "$TIMESTAMP_FILE" ]; then
    LAST_TRANSLATION=$(cat "$TIMESTAMP_FILE")
else
    LAST_TRANSLATION=0
fi

# æŸ¥æ‰¾éœ€è¦æ›´æ–°çš„æ–‡ä»¶ Find files that need updating
find "$SOURCE_DIR" -name "*.md" -newer "$TIMESTAMP_FILE" 2>/dev/null | while read file; do
    rel_path="${file#$SOURCE_DIR/}"
    target_file="$TARGET_DIR/${rel_path%.*}_zh.md"
    
    # åˆ›å»ºç›®æ ‡ç›®å½• Create target directory
    mkdir -p "$(dirname "$target_file")"
    
    echo "Updating: $file -> $target_file"
    markdown-translator -i "$file" -o "$target_file" -c 500 -n 4
done

# æ›´æ–°æ—¶é—´æˆ³ Update timestamp
touch "$TIMESTAMP_FILE"
EOF

chmod +x incremental_translate.sh
./incremental_translate.sh
```

## ğŸš€ CI/CD é›†æˆæœ€ä½³å®è·µ CI/CD Integration Best Practices

### 1. GitHub Actions ä¼˜åŒ– GitHub Actions Optimization

```yaml
# .github/workflows/translate-docs.yml
name: Translate Documentation

on:
  push:
    paths: ['docs/**/*.md']
  pull_request:
    paths: ['docs/**/*.md']

jobs:
  translate:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    strategy:
      matrix:
        # å¹¶è¡Œå¤„ç†ä¸åŒç›®å½• Process different directories in parallel
        directory: ['docs/api', 'docs/guides', 'docs/tutorials']
    
    steps:
    - name: Checkout
      uses: actions/checkout@v4
      with:
        fetch-depth: 2  # è·å–å˜æ›´å†å² Get change history
        
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
        
    - name: Install dependencies
      run: |
        pip install markdown-translator
        
    - name: Get changed files
      id: changed-files
      run: |
        # åªå¤„ç†å˜æ›´çš„æ–‡ä»¶ Only process changed files
        git diff --name-only HEAD~1 HEAD | grep '\.md$' | grep '^${{ matrix.directory }}' > changed_files.txt || true
        echo "files=$(cat changed_files.txt | tr '\n' ' ')" >> $GITHUB_OUTPUT
        
    - name: Translate changed files
      if: steps.changed-files.outputs.files != ''
      env:
        TRANSLATE_API_TOKEN: ${{ secrets.OPENROUTER_API_KEY }}
        TRANSLATE_MODEL: qwen/qwen-2.5-72b-instruct
      run: |
        # å¹¶è¡Œç¿»è¯‘å˜æ›´çš„æ–‡ä»¶ Translate changed files in parallel
        echo "${{ steps.changed-files.outputs.files }}" | xargs -n1 -P4 -I{} \
          markdown-translator -i {} -o {}_zh.md -c 400 -n 2
          
    - name: Commit translations
      if: github.event_name == 'push' && steps.changed-files.outputs.files != ''
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add ${{ matrix.directory }}/**/*_zh.md
        git commit -m "Auto-translate ${{ matrix.directory }} documentation" || exit 0
        git push
```

### 2. è´¨é‡æ£€æŸ¥é›†æˆ Quality Check Integration

```bash
# åˆ›å»ºç¿»è¯‘è´¨é‡æ£€æŸ¥è„šæœ¬ Create translation quality check script
cat > quality_check.sh << 'EOF'
#!/bin/bash

ORIGINAL_FILE="$1"
TRANSLATED_FILE="$2"
QUALITY_REPORT="quality_report.json"

echo "Checking translation quality for: $ORIGINAL_FILE -> $TRANSLATED_FILE"

# åŸºæœ¬æ£€æŸ¥ Basic checks
ORIGINAL_LINES=$(wc -l < "$ORIGINAL_FILE")
TRANSLATED_LINES=$(wc -l < "$TRANSLATED_FILE")
LINE_DIFF_PERCENT=$(echo "scale=2; abs($TRANSLATED_LINES - $ORIGINAL_LINES) / $ORIGINAL_LINES * 100" | bc -l)

# æ£€æŸ¥ä»£ç å—å®Œæ•´æ€§ Check code block integrity
ORIGINAL_CODE_BLOCKS=$(grep -c '```' "$ORIGINAL_FILE")
TRANSLATED_CODE_BLOCKS=$(grep -c '```' "$TRANSLATED_FILE")

# æ£€æŸ¥é“¾æ¥å®Œæ•´æ€§ Check link integrity
ORIGINAL_LINKS=$(grep -o '\[.*\](.*)'  "$ORIGINAL_FILE" | wc -l)
TRANSLATED_LINKS=$(grep -o '\[.*\](.*)'  "$TRANSLATED_FILE" | wc -l)

# ç”Ÿæˆè´¨é‡æŠ¥å‘Š Generate quality report
cat > "$QUALITY_REPORT" << EOF
{
  "original_file": "$ORIGINAL_FILE",
  "translated_file": "$TRANSLATED_FILE",
  "timestamp": "$(date -Iseconds)",
  "metrics": {
    "line_count_diff_percent": $LINE_DIFF_PERCENT,
    "code_blocks_preserved": $([ $ORIGINAL_CODE_BLOCKS -eq $TRANSLATED_CODE_BLOCKS ] && echo true || echo false),
    "links_preserved": $([ $ORIGINAL_LINKS -eq $TRANSLATED_LINKS ] && echo true || echo false)
  },
  "details": {
    "original_lines": $ORIGINAL_LINES,
    "translated_lines": $TRANSLATED_LINES,
    "original_code_blocks": $ORIGINAL_CODE_BLOCKS,
    "translated_code_blocks": $TRANSLATED_CODE_BLOCKS,
    "original_links": $ORIGINAL_LINKS,
    "translated_links": $TRANSLATED_LINKS
  }
}
EOF

# è´¨é‡è¯„åˆ† Quality scoring
QUALITY_SCORE=100
if (( $(echo "$LINE_DIFF_PERCENT > 20" | bc -l) )); then
    QUALITY_SCORE=$((QUALITY_SCORE - 30))
fi
if [ $ORIGINAL_CODE_BLOCKS -ne $TRANSLATED_CODE_BLOCKS ]; then
    QUALITY_SCORE=$((QUALITY_SCORE - 25))
fi
if [ $ORIGINAL_LINKS -ne $TRANSLATED_LINKS ]; then
    QUALITY_SCORE=$((QUALITY_SCORE - 20))
fi

echo "Quality Score: $QUALITY_SCORE/100"

# å¦‚æœè´¨é‡åˆ†æ•°å¤ªä½åˆ™å¤±è´¥ Fail if quality score is too low
if [ $QUALITY_SCORE -lt 70 ]; then
    echo "Quality check failed! Score: $QUALITY_SCORE"
    exit 1
fi

echo "Quality check passed! Score: $QUALITY_SCORE"
EOF

chmod +x quality_check.sh
```

## ğŸ“ˆ æˆæœ¬ä¼˜åŒ– Cost Optimization

### 1. æ¨¡å‹é€‰æ‹©ç­–ç•¥ Model Selection Strategy

```bash
# æ ¹æ®å†…å®¹é‡è¦æ€§é€‰æ‹©æ¨¡å‹ Choose model based on content importance
classify_content() {
    local file="$1"
    local filename=$(basename "$file")
    
    # é«˜ä¼˜å…ˆçº§æ–‡ä»¶ High priority files
    if [[ "$filename" =~ ^(README|CHANGELOG|LICENSE|CONTRIBUTING) ]]; then
        echo "claude-3-5-sonnet-20241022"
    # APIæ–‡æ¡£ API documentation
    elif [[ "$file" =~ /api/ ]]; then
        echo "qwen/qwen-2.5-72b-instruct"
    # å†…éƒ¨æ–‡æ¡£ Internal documentation
    elif [[ "$file" =~ /(internal|draft)/ ]]; then
        echo "qwen/qwen-2.5-7b-instruct"
    # é»˜è®¤ Default
    else
        echo "qwen/qwen-2.5-72b-instruct"
    fi
}

# ä½¿ç”¨åˆ†ç±»ç»“æœ Use classification result
INPUT_FILE="$1"
SELECTED_MODEL=$(classify_content "$INPUT_FILE")
export TRANSLATE_MODEL="$SELECTED_MODEL"

echo "Using model $SELECTED_MODEL for $INPUT_FILE"
markdown-translator -i "$INPUT_FILE"
```

### 2. æ‰¹é‡æŠ˜æ‰£ä¼˜åŒ– Bulk Discount Optimization

```bash
# æ‰¹é‡å¤„ç†ä»¥è·å¾—æ›´å¥½çš„æˆæœ¬æ•ˆç›Š Batch processing for better cost efficiency
batch_translate() {
    local files=("$@")
    local batch_size=10
    
    for ((i=0; i<${#files[@]}; i+=batch_size)); do
        local batch=("${files[@]:i:batch_size}")
        
        echo "Processing batch $((i/batch_size + 1)): ${#batch[@]} files"
        
        # å¹¶è¡Œå¤„ç†æ‰¹æ¬¡ Process batch in parallel
        printf '%s\n' "${batch[@]}" | xargs -n1 -P4 -I{} \
            markdown-translator -i {} -c 800 -n 2
        
        # æ‰¹æ¬¡é—´çŸ­æš‚æš‚åœ Brief pause between batches
        sleep 2
    done
}

# æ”¶é›†æ‰€æœ‰éœ€è¦ç¿»è¯‘çš„æ–‡ä»¶ Collect all files to translate
mapfile -t files < <(find docs -name "*.md" -type f)
batch_translate "${files[@]}"
```

---

**ğŸ’¡ å…³é”®è¦ç‚¹ Key Takeaways**:

1. **è´¨é‡ä¼˜å…ˆ Quality First**: ä¸ºé‡è¦æ–‡æ¡£é€‰æ‹©é«˜è´¨é‡æ¨¡å‹
2. **æ€§èƒ½å¹³è¡¡ Performance Balance**: æ ¹æ®ç½‘ç»œå’Œç³»ç»Ÿèµ„æºè°ƒæ•´å¹¶å‘å‚æ•°
3. **å®‰å…¨ç¬¬ä¸€ Security First**: å¦¥å–„ç®¡ç†APIå¯†é’¥å’ŒéªŒè¯è¾“å…¥
4. **ç›‘æ§é‡è¦ Monitoring Matters**: å®æ–½å…¨é¢çš„æ—¥å¿—å’Œç›‘æ§
5. **æˆæœ¬æ„è¯† Cost Conscious**: æ ¹æ®å†…å®¹é‡è¦æ€§é€‰æ‹©åˆé€‚çš„æ¨¡å‹

éµå¾ªè¿™äº›æœ€ä½³å®è·µå°†å¸®åŠ©æ‚¨è·å¾—æœ€ä½³çš„ç¿»è¯‘æ•ˆæœå’Œä½¿ç”¨ä½“éªŒã€‚

Following these best practices will help you achieve optimal translation results and user experience.
